//importing packages
import java.io.*;
import org.apache.hadoop.conf.Configuration;
import java.uti.StringTokenizer;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount{

public static class TokenizerMapper extends Mapper<Object,Text,Text,IntWritable>
{
private final static IntWritable one = new IntWritable(1);
private Text word = new Text();

public void map(Object key, Text Value, Context context)throws IOException, Interrupted Exception
{
StringTokenizer itr = new StringTokenizer(Value.toString());
while(itr.hasMoreTokens())
{
word.set(itr.nextToken());
context.write(word,one);
}

}
}
public static class SumReducer extends Reducer<Text,IntWritable,Text,IntWritable>
{
private IntWritable result= new IntWritable();

public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException
{
int sum=0;
for(IntWritable val:values)
{
sum=sum+val.get();
}
result.set(sum);
context.write(key,result);
}
}
}












